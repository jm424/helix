//! Controller partition for cluster metadata coordination.
//!
//! The controller partition (group ID 0) is a Raft group that ALL nodes participate in.
//! It stores metadata commands and ensures consistent cluster state across all nodes:
//! - Topic creation and deletion
//! - Partition assignments (which nodes host which partitions)
//! - Partition leadership tracking
//!
//! This follows the Redpanda controller partition pattern.

use std::collections::HashMap;

use bytes::{Buf, BufMut, Bytes, BytesMut};
use helix_core::{GroupId, NodeId, PartitionId, TopicId};

/// Controller partition group ID (always 0).
pub const CONTROLLER_GROUP_ID: GroupId = GroupId::new(0);

// -----------------------------------------------------------------------------
// ControllerCommand
// -----------------------------------------------------------------------------

/// Commands that can be proposed to the controller partition.
///
/// These commands are replicated via Raft to all nodes, ensuring consistent
/// cluster metadata across the cluster.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ControllerCommand {
    /// Create a new topic.
    ///
    /// When committed, the controller leader generates `AssignPartition` commands
    /// for each partition based on the replication factor.
    CreateTopic {
        /// Topic name.
        name: String,
        /// Number of partitions.
        partition_count: u32,
        /// Replication factor (number of replicas per partition).
        replication_factor: u32,
    },

    /// Delete a topic and all its partitions.
    DeleteTopic {
        /// Topic name to delete.
        name: String,
    },

    /// Assign a partition to a set of nodes.
    ///
    /// This is an internal command generated by the controller leader
    /// when processing `CreateTopic`. It specifies which nodes should
    /// host replicas of this partition.
    AssignPartition {
        /// Topic ID (allocated when topic is created).
        topic_id: TopicId,
        /// Partition ID within the topic.
        partition_id: PartitionId,
        /// Raft group ID for this partition's data replication.
        group_id: GroupId,
        /// Nodes that should host replicas (first is preferred leader).
        replicas: Vec<NodeId>,
    },

    /// Update the leader for a partition.
    ///
    /// This is proposed when a node detects it has become leader for a partition
    /// via Raft election, to update the cluster-wide view.
    UpdatePartitionLeader {
        /// Topic ID.
        topic_id: TopicId,
        /// Partition ID.
        partition_id: PartitionId,
        /// New leader node ID (None if no leader).
        leader: Option<NodeId>,
    },
}

/// Command type tags for encoding.
const CMD_CREATE_TOPIC: u8 = 1;
const CMD_DELETE_TOPIC: u8 = 2;
const CMD_ASSIGN_PARTITION: u8 = 3;
const CMD_UPDATE_PARTITION_LEADER: u8 = 4;

impl ControllerCommand {
    /// Encodes the command to bytes for Raft replication.
    #[must_use]
    pub fn encode(&self) -> Bytes {
        let mut buf = BytesMut::new();

        match self {
            Self::CreateTopic {
                name,
                partition_count,
                replication_factor,
            } => {
                buf.put_u8(CMD_CREATE_TOPIC);
                // Encode name as length-prefixed string.
                let name_bytes = name.as_bytes();
                #[allow(clippy::cast_possible_truncation)]
                buf.put_u32_le(name_bytes.len() as u32);
                buf.put_slice(name_bytes);
                buf.put_u32_le(*partition_count);
                buf.put_u32_le(*replication_factor);
            }

            Self::DeleteTopic { name } => {
                buf.put_u8(CMD_DELETE_TOPIC);
                let name_bytes = name.as_bytes();
                #[allow(clippy::cast_possible_truncation)]
                buf.put_u32_le(name_bytes.len() as u32);
                buf.put_slice(name_bytes);
            }

            Self::AssignPartition {
                topic_id,
                partition_id,
                group_id,
                replicas,
            } => {
                buf.put_u8(CMD_ASSIGN_PARTITION);
                buf.put_u64_le(topic_id.get());
                buf.put_u64_le(partition_id.get());
                buf.put_u64_le(group_id.get());
                // Encode replicas as count + node IDs.
                #[allow(clippy::cast_possible_truncation)]
                buf.put_u32_le(replicas.len() as u32);
                for node_id in replicas {
                    buf.put_u64_le(node_id.get());
                }
            }

            Self::UpdatePartitionLeader {
                topic_id,
                partition_id,
                leader,
            } => {
                buf.put_u8(CMD_UPDATE_PARTITION_LEADER);
                buf.put_u64_le(topic_id.get());
                buf.put_u64_le(partition_id.get());
                match leader {
                    Some(node_id) => {
                        buf.put_u8(1); // Has leader.
                        buf.put_u64_le(node_id.get());
                    }
                    None => {
                        buf.put_u8(0); // No leader.
                    }
                }
            }
        }

        buf.freeze()
    }

    /// Decodes a command from bytes.
    ///
    /// Returns `None` if the data is invalid or incomplete.
    #[must_use]
    pub fn decode(data: &Bytes) -> Option<Self> {
        if data.is_empty() {
            return None;
        }

        let mut buf = data.clone();
        let cmd_type = buf.get_u8();

        match cmd_type {
            CMD_CREATE_TOPIC => {
                if buf.remaining() < 4 {
                    return None;
                }
                let name_len = buf.get_u32_le() as usize;
                if buf.remaining() < name_len + 8 {
                    return None;
                }
                let name = String::from_utf8(buf.copy_to_bytes(name_len).to_vec()).ok()?;
                let partition_count = buf.get_u32_le();
                let replication_factor = buf.get_u32_le();

                Some(Self::CreateTopic {
                    name,
                    partition_count,
                    replication_factor,
                })
            }

            CMD_DELETE_TOPIC => {
                if buf.remaining() < 4 {
                    return None;
                }
                let name_len = buf.get_u32_le() as usize;
                if buf.remaining() < name_len {
                    return None;
                }
                let name = String::from_utf8(buf.copy_to_bytes(name_len).to_vec()).ok()?;

                Some(Self::DeleteTopic { name })
            }

            CMD_ASSIGN_PARTITION => {
                if buf.remaining() < 24 + 4 {
                    return None;
                }
                let topic_id = TopicId::new(buf.get_u64_le());
                let partition_id = PartitionId::new(buf.get_u64_le());
                let group_id = GroupId::new(buf.get_u64_le());
                let replica_count = buf.get_u32_le() as usize;

                if buf.remaining() < replica_count * 8 {
                    return None;
                }
                let mut replicas = Vec::with_capacity(replica_count);
                for _ in 0..replica_count {
                    replicas.push(NodeId::new(buf.get_u64_le()));
                }

                Some(Self::AssignPartition {
                    topic_id,
                    partition_id,
                    group_id,
                    replicas,
                })
            }

            CMD_UPDATE_PARTITION_LEADER => {
                if buf.remaining() < 16 + 1 {
                    return None;
                }
                let topic_id = TopicId::new(buf.get_u64_le());
                let partition_id = PartitionId::new(buf.get_u64_le());
                let has_leader = buf.get_u8();
                let leader = if has_leader == 1 {
                    if buf.remaining() < 8 {
                        return None;
                    }
                    Some(NodeId::new(buf.get_u64_le()))
                } else {
                    None
                };

                Some(Self::UpdatePartitionLeader {
                    topic_id,
                    partition_id,
                    leader,
                })
            }

            _ => None, // Unknown command type.
        }
    }
}

// -----------------------------------------------------------------------------
// ControllerState
// -----------------------------------------------------------------------------

/// Information about a topic.
#[derive(Debug, Clone)]
pub struct TopicInfo {
    /// Unique topic ID.
    pub topic_id: TopicId,
    /// Topic name.
    pub name: String,
    /// Number of partitions.
    pub partition_count: u32,
    /// Replication factor.
    pub replication_factor: u32,
}

/// Assignment of a partition to nodes.
#[derive(Debug, Clone)]
pub struct PartitionAssignment {
    /// Raft group ID for data replication.
    pub group_id: GroupId,
    /// Nodes hosting replicas (first is preferred leader).
    pub replicas: Vec<NodeId>,
    /// Current leader (if known).
    pub leader: Option<NodeId>,
}

/// Controller state machine.
///
/// This state is rebuilt by replaying committed controller commands.
/// All nodes maintain this state, ensuring consistent cluster metadata.
#[derive(Debug, Default)]
pub struct ControllerState {
    /// Topics by name.
    topics: HashMap<String, TopicInfo>,
    /// Topics by ID (for reverse lookup).
    topics_by_id: HashMap<TopicId, String>,
    /// Partition assignments: (`topic_id`, `partition_id`) -> assignment.
    assignments: HashMap<(TopicId, PartitionId), PartitionAssignment>,
    /// Next topic ID to allocate.
    next_topic_id: u64,
    /// Next group ID to allocate (starts at 1, since 0 is controller).
    next_group_id: u64,
}

impl ControllerState {
    /// Creates a new empty controller state.
    #[must_use]
    pub fn new() -> Self {
        Self {
            topics: HashMap::new(),
            topics_by_id: HashMap::new(),
            assignments: HashMap::new(),
            next_topic_id: 1,
            next_group_id: 1, // 0 is reserved for controller partition.
        }
    }

    /// Applies a controller command to update state.
    ///
    /// Returns commands to be proposed (for `CreateTopic`, generates `AssignPartition` commands).
    /// The caller should propose these follow-up commands if this node is the controller leader.
    #[must_use]
    pub fn apply(
        &mut self,
        command: &ControllerCommand,
        cluster_nodes: &[NodeId],
    ) -> Vec<ControllerCommand> {
        let mut follow_up_commands = Vec::new();

        match command {
            ControllerCommand::CreateTopic {
                name,
                partition_count,
                replication_factor,
            } => {
                // Skip if topic already exists.
                if self.topics.contains_key(name) {
                    return follow_up_commands;
                }

                // Allocate topic ID.
                let topic_id = TopicId::new(self.next_topic_id);
                self.next_topic_id += 1;

                // Store topic info.
                let topic_info = TopicInfo {
                    topic_id,
                    name: name.clone(),
                    partition_count: *partition_count,
                    replication_factor: *replication_factor,
                };
                self.topics.insert(name.clone(), topic_info);
                self.topics_by_id.insert(topic_id, name.clone());

                // Generate partition assignments.
                // Controller leader should propose these as follow-up commands.
                for partition in 0..*partition_count {
                    let partition_id = PartitionId::new(u64::from(partition));
                    let group_id = GroupId::new(self.next_group_id);
                    self.next_group_id += 1;

                    // Round-robin replica assignment.
                    let replicas = Self::assign_replicas(
                        cluster_nodes,
                        partition,
                        *replication_factor,
                    );

                    follow_up_commands.push(ControllerCommand::AssignPartition {
                        topic_id,
                        partition_id,
                        group_id,
                        replicas,
                    });
                }
            }

            ControllerCommand::DeleteTopic { name } => {
                if let Some(topic_info) = self.topics.remove(name) {
                    self.topics_by_id.remove(&topic_info.topic_id);
                    // Remove all partition assignments for this topic.
                    self.assignments.retain(|(tid, _), _| *tid != topic_info.topic_id);
                }
            }

            ControllerCommand::AssignPartition {
                topic_id,
                partition_id,
                group_id,
                replicas,
            } => {
                self.assignments.insert(
                    (*topic_id, *partition_id),
                    PartitionAssignment {
                        group_id: *group_id,
                        replicas: replicas.clone(),
                        leader: None, // Leader will be elected by Raft.
                    },
                );
            }

            ControllerCommand::UpdatePartitionLeader {
                topic_id,
                partition_id,
                leader,
            } => {
                if let Some(assignment) = self.assignments.get_mut(&(*topic_id, *partition_id)) {
                    assignment.leader = *leader;
                }
            }
        }

        follow_up_commands
    }

    /// Assigns replicas for a partition using round-robin distribution.
    ///
    /// For partition P with replication factor R:
    /// - Start at node (P % N) where N is node count
    /// - Take R consecutive nodes (wrapping around)
    ///
    /// This spreads leaders (first replica) evenly across nodes.
    fn assign_replicas(
        cluster_nodes: &[NodeId],
        partition: u32,
        replication_factor: u32,
    ) -> Vec<NodeId> {
        let node_count = cluster_nodes.len();
        if node_count == 0 {
            return Vec::new();
        }

        let start_idx = (partition as usize) % node_count;
        let replica_count = (replication_factor as usize).min(node_count);

        let mut replicas = Vec::with_capacity(replica_count);
        for i in 0..replica_count {
            let idx = (start_idx + i) % node_count;
            replicas.push(cluster_nodes[idx]);
        }

        replicas
    }

    // -------------------------------------------------------------------------
    // Query methods
    // -------------------------------------------------------------------------

    /// Returns topic info by name.
    #[must_use]
    pub fn get_topic(&self, name: &str) -> Option<&TopicInfo> {
        self.topics.get(name)
    }

    /// Returns topic info by ID.
    #[must_use]
    pub fn get_topic_by_id(&self, topic_id: TopicId) -> Option<&TopicInfo> {
        self.topics_by_id
            .get(&topic_id)
            .and_then(|name| self.topics.get(name))
    }

    /// Returns partition assignment.
    #[must_use]
    pub fn get_assignment(
        &self,
        topic_id: TopicId,
        partition_id: PartitionId,
    ) -> Option<&PartitionAssignment> {
        self.assignments.get(&(topic_id, partition_id))
    }

    /// Returns all topics.
    pub fn topics(&self) -> impl Iterator<Item = &TopicInfo> {
        self.topics.values()
    }

    /// Returns all assignments for a topic.
    pub fn topic_assignments(
        &self,
        topic_id: TopicId,
    ) -> impl Iterator<Item = (PartitionId, &PartitionAssignment)> {
        self.assignments
            .iter()
            .filter(move |((tid, _), _)| *tid == topic_id)
            .map(|((_, pid), assignment)| (*pid, assignment))
    }

    /// Checks if a topic exists.
    #[must_use]
    pub fn topic_exists(&self, name: &str) -> bool {
        self.topics.contains_key(name)
    }

    /// Returns the group ID for a partition, if assigned.
    #[must_use]
    pub fn get_group_id(
        &self,
        topic_id: TopicId,
        partition_id: PartitionId,
    ) -> Option<GroupId> {
        self.assignments
            .get(&(topic_id, partition_id))
            .map(|a| a.group_id)
    }

    /// Returns all partitions assigned to a specific node.
    #[must_use]
    pub fn partitions_for_node(
        &self,
        node_id: NodeId,
    ) -> Vec<(TopicId, PartitionId, &PartitionAssignment)> {
        self.assignments
            .iter()
            .filter(|(_, assignment)| assignment.replicas.contains(&node_id))
            .map(|((topic_id, partition_id), assignment)| (*topic_id, *partition_id, assignment))
            .collect()
    }
}

// -----------------------------------------------------------------------------
// Tests
// -----------------------------------------------------------------------------

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_command_encode_decode_create_topic() {
        let cmd = ControllerCommand::CreateTopic {
            name: "test-topic".to_string(),
            partition_count: 3,
            replication_factor: 2,
        };

        let encoded = cmd.encode();
        let decoded = ControllerCommand::decode(&encoded).unwrap();

        assert_eq!(cmd, decoded);
    }

    #[test]
    fn test_command_encode_decode_delete_topic() {
        let cmd = ControllerCommand::DeleteTopic {
            name: "test-topic".to_string(),
        };

        let encoded = cmd.encode();
        let decoded = ControllerCommand::decode(&encoded).unwrap();

        assert_eq!(cmd, decoded);
    }

    #[test]
    fn test_command_encode_decode_assign_partition() {
        let cmd = ControllerCommand::AssignPartition {
            topic_id: TopicId::new(1),
            partition_id: PartitionId::new(0),
            group_id: GroupId::new(5),
            replicas: vec![NodeId::new(1), NodeId::new(2), NodeId::new(3)],
        };

        let encoded = cmd.encode();
        let decoded = ControllerCommand::decode(&encoded).unwrap();

        assert_eq!(cmd, decoded);
    }

    #[test]
    fn test_command_encode_decode_update_leader() {
        let cmd = ControllerCommand::UpdatePartitionLeader {
            topic_id: TopicId::new(1),
            partition_id: PartitionId::new(0),
            leader: Some(NodeId::new(2)),
        };

        let encoded = cmd.encode();
        let decoded = ControllerCommand::decode(&encoded).unwrap();

        assert_eq!(cmd, decoded);

        // Test with no leader.
        let cmd_no_leader = ControllerCommand::UpdatePartitionLeader {
            topic_id: TopicId::new(1),
            partition_id: PartitionId::new(0),
            leader: None,
        };

        let encoded = cmd_no_leader.encode();
        let decoded = ControllerCommand::decode(&encoded).unwrap();

        assert_eq!(cmd_no_leader, decoded);
    }

    #[test]
    fn test_controller_state_create_topic() {
        let mut state = ControllerState::new();
        let nodes = vec![NodeId::new(1), NodeId::new(2), NodeId::new(3)];

        let cmd = ControllerCommand::CreateTopic {
            name: "test-topic".to_string(),
            partition_count: 3,
            replication_factor: 2,
        };

        let follow_ups = state.apply(&cmd, &nodes);

        // Should generate 3 AssignPartition commands.
        assert_eq!(follow_ups.len(), 3);

        // Topic should exist.
        assert!(state.topic_exists("test-topic"));
        let topic = state.get_topic("test-topic").unwrap();
        assert_eq!(topic.partition_count, 3);
        assert_eq!(topic.replication_factor, 2);
    }

    #[test]
    fn test_controller_state_assign_partition() {
        let mut state = ControllerState::new();
        let nodes = vec![NodeId::new(1), NodeId::new(2), NodeId::new(3)];

        // First create the topic.
        let create_cmd = ControllerCommand::CreateTopic {
            name: "test-topic".to_string(),
            partition_count: 1,
            replication_factor: 2,
        };
        let follow_ups = state.apply(&create_cmd, &nodes);

        // Apply the generated AssignPartition command.
        assert_eq!(follow_ups.len(), 1);
        let _ = state.apply(&follow_ups[0], &nodes);

        // Check assignment exists.
        let topic = state.get_topic("test-topic").unwrap();
        let assignment = state.get_assignment(topic.topic_id, PartitionId::new(0)).unwrap();
        assert_eq!(assignment.replicas.len(), 2);
        assert_eq!(assignment.group_id.get(), 1); // First data group.
    }

    #[test]
    fn test_replica_assignment_distribution() {
        let nodes = vec![NodeId::new(1), NodeId::new(2), NodeId::new(3)];

        // Partition 0 should start at node 0 (node 1).
        let replicas0 = ControllerState::assign_replicas(&nodes, 0, 2);
        assert_eq!(replicas0, vec![NodeId::new(1), NodeId::new(2)]);

        // Partition 1 should start at node 1 (node 2).
        let replicas1 = ControllerState::assign_replicas(&nodes, 1, 2);
        assert_eq!(replicas1, vec![NodeId::new(2), NodeId::new(3)]);

        // Partition 2 should start at node 2 (node 3), wrapping to node 1.
        let replicas2 = ControllerState::assign_replicas(&nodes, 2, 2);
        assert_eq!(replicas2, vec![NodeId::new(3), NodeId::new(1)]);
    }

    #[test]
    fn test_delete_topic() {
        let mut state = ControllerState::new();
        let nodes = vec![NodeId::new(1), NodeId::new(2)];

        // Create topic.
        let create_cmd = ControllerCommand::CreateTopic {
            name: "test-topic".to_string(),
            partition_count: 2,
            replication_factor: 2,
        };
        let follow_ups = state.apply(&create_cmd, &nodes);
        for cmd in &follow_ups {
            let _ = state.apply(cmd, &nodes);
        }

        assert!(state.topic_exists("test-topic"));
        assert_eq!(state.assignments.len(), 2);

        // Delete topic.
        let delete_cmd = ControllerCommand::DeleteTopic {
            name: "test-topic".to_string(),
        };
        let _ = state.apply(&delete_cmd, &nodes);

        assert!(!state.topic_exists("test-topic"));
        assert_eq!(state.assignments.len(), 0);
    }
}
